# Language

1. **Introduction**

   - What is natural language processing (NLP)?
   - Current state-of-the-art of NLP
   - Ambiguity
   - Other challenges
   - Representing words, phrases and sentences

2. **Segmentation and tokenization**

   - Regular expressions
   - Word tokenization, lemmatization and stemming
   - Sentence segmentation
   - Subword tokenization

3. **Language Modelling**

   - N-gram language models
   - perplexity
   - maximum likelihood estimation
   - smoothing

4. **Neural Language Modelling**

   - Word embeddings
   - Vector space models for NLP
   - Recurrent neural network (RNN) for language modelling
   - Transformer architecture for language modelling
   - Use of language models in downstream tasks: fine-tuning and pretraining

5. **Part-of-Speech (POS) Tagging**

   - Hidden Markov model and viterbi
   - Conditional Random Fields
   - (Bi)LSTM for POS tagging
   - Encoder-decoder architecture for sequence-to-sequence labeling

6. **Morphological analysis**

   - Inflection and derivation
   - Finite state morphology
   - Sequence-to-sequence neural models of morphological inflection

7. **Syntactic Parsing**

   - Universal Dependencies
   - Dependency parsing: Graph based dependency parsing, transition based dependency parsing
   - Constituent parsing with a (probabilistic) context free grammar ((P)CFG) and the Cocke-Younger-Kasami (CYK) algorithm

8. **Semantics (lexical and compositional)**

   - Word sense disambiguation
   - Semantic role labelling

9. **Discourse: Coreference Resolution**

   - Discourse coherence
   - Algorithm of Hobbs
   - Neural end-to-end coreference resolution

10. **Question Answering**

    - Evolution of QA systems from rule-based to neural
    - Complex pipelines to end-to-end to retrieval-free
    - Closed-domain vs open-domain
    - Text-only vs multimodal

11. **Neural Machine Translation**

    - Encoder-decoder architecture (e.g., RNN, transformer-based)
    - Attention models
    - Improvements and alternative architectures that deal with limited parallel training data

12. **Conversational Dialogue Systems and Chatbots**

    - Task oriented dialog agents: Rule based versus neural based approaches
    - Chatbots: End-to-end sequence-to-sequence neural models

## References

Jurafsky, D., & Martin, J. H. (2024). Speech and language processing (3rd ed., draft). Retrieved from <https://web.stanford.edu/~jurafsky/slp3/>

Malan, D., & Yu, B. (2024). CS50â€™s Introduction to Artificial Intelligence with Python [Course materials]. Harvard OpenCourseWare. Retrieved from <https://cs50.harvard.edu/ai/2024/notes/6/>
